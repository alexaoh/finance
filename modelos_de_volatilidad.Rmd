---
title: "NASDAQ Composite Index"
subtitle: "Final Project - Volatility Models - Financial Statistics"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document:
  #   code_folding: show
  #   toc: true
  #   toc_depth: 3
  #   theme: readable
  #   highlight: textmate
  #   number_sections: true
  pdf_document:
    fig_caption: true
    number_sections: true
    toc: false
editor_options: 
  chunk_output_type: console
geometry:
  margin = 2.2cm
urlcolor: blue
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = T, warning = F, fig.width = 10, comment = "#>", size = "footnotesize")
setwd("/home/ajo/gitRepos/finance")
library(quantmod)
library(fBasics)
library(urca)
library(forecast)
library(rugarch)
library(fTrading)
library(latex2exp)
library(ggplot2)
library(tidyverse)
library(rmgarch)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

# Abstract
The NASDAQ Composite Index (IXIC) is studied using data from 01.01.2012 to 01.03.2022. The mean and conditional variance of the series are modelized. The work shows that (SUMMARY 100 words).

MAKE REFERENCES BETWEEN PLOTS (NUMBERS AND CITATIONS IN TEXT, bookdown is necessary maybe?) LATER. 

\newpage
# Introduction 
Describe

* Scenario and objective of the work. What will be analyzed. 
* Precise description of variable (NASDAQ Composite) used in the analysis and description of where the data is gathered from (Yahoo Finance)
* Summary of structure of the work (description of what is done in each part)

The NASDAQ Composite Index (IXIC) is analyzed from 01.01.2012 to 01.03.2022 (2556 days of data). The data is downloaded from Yahoo Finance and can be found [here](https://finance.yahoo.com/quote/%5EIXIC?p=%5EIXIC). As NASDAQ explains in this [article](https://www.nasdaq.com/articles/what-is-the-nasdaq-composite-and-what-companies-are-in-it-2021-05-12) "The Nasdaq Composite Index, popularly referred to as ‘The Nasdaq’ by the media, covers more than 3,000 stocks, all of which are listed on the Nasdaq Stock Market". It is a market-cap weighted index, such that it represents the value of all its listed stocks. Moreover, technology dominates almost half of the composite weight. 

As noted, the data is downloaded from [Yahoo Finance](https://finance.yahoo.com/). This is a free portal that aggregates financial information like market news, stock prices, personal finance information, portfolio management resources and much more. 

The mean and conditional variance of the financial time series are modelized, in order to study its volatility. The volatility models can be used to learn several things about the index. First of all, they can be used to predict and interpret future volatility. Additionally, they can be used to interpret the impact of news on the index. Moreover, they can be used to calculate the Value at Risk (VaR). All of these applications are shown in this work. Finally, a multivariate analysis is done, explicitly including the stock of Stratus Properties Inc. (STRS), in order to study some multivariate properties between the two financial time series. Note that STRS is one of the top [30 components](https://finance.yahoo.com/quote/%5EIXIC/components?p=%5EIXIC) of IXIC. 

The table of contents is shown below. The rest of the report is split into a univariate part and a multivariate part, where the univariate part is the largest and most detailed. Part 3.1 loads and describes the data in a concise fashion, detailing some events that may be related to the changes in the daily adjusted closing price. Section 3.2 analyzes the stationarity of the series, which leads to the conclusion that IXIC is in fact non-stationary. The returns of the series are stationary however, which means that they are used in the remainder of the work instead. Section 3.3 presents some basic statistical properties of the returns. Section 3.4 and 3.5 build models for the mean and variance, respectively, of the stationary series. Section 3.6 presents a grafic and interpretation of the volatility series estimated by the model found in previous sections, while section 3.7 shows the news impact curve of the modelized series. Part 3.8 does volatility predictions and interpretations, based on the models estimated previously. Section 3.9 calculates volatility with two other methods which have not been used earlier in the work; historical volatility and Exponentially Weighted Moving Average (EWMA). Finally in section 3, part 3.10 calculates and interprets the Value at Risk (VaR). The second main part of the report treats a multivariate analysis of IXIC, coupled with STRS. In section 4.1 a multivariate DCC GARCH model is fitted to the residuals of the time series. In this case, the identification, estimation and diagnostics for the models of for the mean and variance of the residuals of STRS is not shown. Section 4.2 estimates the correlation between the two financial series and shows the news impact surface, which is the bivariate equivalent to the news impact curve. Finally, a conclusion of the work is formulated. 

\newpage
\tableofcontents

\newpage
# Univariate Analysis

## Description of Data
First, we load the NASDAQ Composite Index data from Yahoo Finance. Note that I downloaded the data in csv format instead of loading directly via the `quantmod` `getSymbols` API, in order to make sure that I always have access to the data. 

```{r, echo = F}
options("getSymbols.warning4.0"=FALSE)
```


```{r}
#getSymbols("^IXIC",from="2012-01-01", to="2022-03-01", warnings = F) 
Ixic <- read.csv("IXIC.csv", )
dim(Ixic) 
any(is.na(Ixic))
#dim(IXIC)
# Want the adjusted closing price. 
ixic <- Ixic[,6]
#IXIC <- IXIC[,6]
```

The data does not have any NA values (weekends and holidays have been removed already), such that we can start working with the data without the need to replace missing values. The series is plotted below. 

```{r, echo = F}
tibble("date" = as.Date(Ixic[,1]), ixic) %>% ggplot(aes(x = date, y = ixic)) +
    geom_line() +
    theme_minimal() +
    scale_x_date(date_labels = "%Y %b %d", date_breaks = "1 year") +
    ggtitle("NASDAQ Composite (^IXIC)")
```

We note some important moments during the time in question. The index fell in January 2016, perhaps in relation to the [2015-2016 stock market sellof](https://en.wikipedia.org/wiki/2015%E2%80%932016_stock_market_selloff) or a slowdown in China and falling oil prices, as noted [here](https://money.cnn.com/2016/01/29/investing/dow-january-2016-worst-month/). In January 2019 there was a stock market crash following an [announcement](https://finance.yahoo.com/news/stock-market-news-jan-4-145702705.html) from Apple's CEO Tim Cook. Moreover, the market slump was dependent on weak Chinese manufacturing data, as noted [here](https://www.theguardian.com/business/live/2019/jan/02/stocks-start-2019-in-retreat-as-china-factory-output-shrinks-business-live). The relatively large fall in price in the beginning of 2020 was a result of the spread of COVID-19. This event leads up to the fall in the beginning of 2022, when Russia eventually launched an invasion on Ukraine on February 24. 

## Analysis of Stationarity
In order to see if the series is stationary, we will employ both informal and formal tests. Immediately, by looking at the plot of the series, it does not look stationary, since the mean of the process looks to change quite dramatically with time. Some more informal tests are done. The function of autocorrelation and partial autocorrelation (empirical) for the series are plotted below. 

```{r, echo = F}
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1) 
acf(ixic,ylim=c(-1,1),main="ixic")
pacf(ixic,ylim=c(-1,1),main="ixic")
```

As is seen from the function of autocorrelation (ACF), the coefficients decrease slowly towards zero. This suggests that the time series is non-stationary, since a stationary series would show quickly decreasing autocorrelation coefficients. 

Next, some Ljung-Box tests are done. Here we are testing the joint hypothesis that all $m$ of the correlation coefficients are simultaneously equal to zero. Below we are testing for $m \in \{1, 5, 10, 15, 20\}$. Only the first output is shown, because all of them give very low $p$-values. 

```{r}
Box.test(ixic, lag = 1, type = c("Ljung-Box"))
```

```{r, eval = F}
Box.test(ixic, lag = 5, type = c("Ljung-Box"))
Box.test(ixic, lag = 10, type = c("Ljung-Box"))
Box.test(ixic, lag = 15, type = c("Ljung-Box"))
Box.test(ixic, lag = 20, type = c("Ljung-Box"))
```

The low $p$-values mean that, to any reasonably chosen significance level (often at 5\%), the null hypothesis that all $m$ correlation coefficients are simultaneously equal to zero is rejected. This further suggests that the series is non-stationary. 

Next, some formal tests are done to check stationarity of the series. First, the Augmented-Dickey-Fuller (ADF) unit root test is done. The null hypothesis for this test states that the series is integrated of order 1, i.e. that it is non-stationary. Below, the ADF test is done assuming both a stochastic and deterministic trend in the data. The maximum number of lags considered are 20 and the number of lags used are chosen by BIC. 

```{r}
ixic.df<-ur.df(ixic, type = c("trend"), lags=20, selectlags = c("BIC"))
summary(ixic.df)	
```

From the output it is apparent that BIC chooses 9 lags in the ADF test. Moreover, the value of the test-statistic clearly suggests that we cannot reject the null-hypothesis, since the value is much larger than the critical values for this left-sided test. Thus, we would conclude that the series is non-stationary. Note that the test leads to the same conclusion when assuming no trends and when assuming only a drift. Moreover, the same amount of lags were chosen automatically for all three variants. Below, the residuals and the autocorrelation functions of the residuals are plotted, in order to check if the number of lags chosen via BIC is satisfactory. 

```{r, echo = F}
plot(ixic.df)
```

The autocorrelation function of the residuals has no significant coefficients, which leads us to conclude that the amount of lags chosen via BIC is satisfactory. 

Next, we check if the returns are stationary. Below we calculate the (logarithmic) returns. Note that the first difference is removed, since it is not a numerical value. 

```{r}
rendixic <- diff(log(ixic))
```

The returns are plotted below. 

```{r, echo = F}
tibble("date" = as.Date(Ixic[-c(1),1]), rendixic) %>% ggplot(aes(x = date, y = rendixic)) +
    geom_line() +
    theme_minimal() +
    scale_x_date(date_labels = "%Y %b %d", date_breaks = "1 year") +
    ggtitle("Returns of NASDAQ Composite (^IXIC)")
```

Then, the ADF test is calculated without trends, since there does not look to be any trends in the plot of the returns. Note that, as earlier, the conclusion of the test and the amount of lags that are chosen via BIC are the same when assuming a drift or both types of trends.

```{r}
rendixic.df<-ur.df(rendixic, type = c("none"), lags=20, selectlags = c("BIC"))
summary(rendixic.df)	
```

It is apparent that 8 lags are chosen. Moreover, from the test-statistic above we would reject the null-hypothesis, which means that we have found evidence against the hypothesis that the returns are $I(1)$. Thus, we conclude that the returns are $I(0)$ or, equivalently, the original series is $I(1)$. This means that the original series is not stationary according to this test as well, but the returns are stationary and can be used in the analysis. 

As earlier, the plot below shows that the amount of lags for the ADF test chosen via BIC is satisfactory. 

```{r, echo = F}
plot(rendixic.df)
```

For completeness, we also use the Philips-Perron (PP) test to check stationarity of the series. This test defines the same null-hypothesis as the ADF test, which means that this also is a left-tailed test. The output from the code blocks below are not printed, as they yield the same results as the ADF test above. 

```{r}
ixic.pp<-ur.pp(ixic, type = c("Z-tau"), model = c("trend"), lags = c("long"))
```

All combinations of trend assumptions and long or short lags yield the same conclusions; we have not found sufficient evidence to reject the null-hypothesis of non-stationarity of the series. Below the PP-test is done with the returns. 

```{r}
rendixic.pp<-ur.pp(rendixic, type = c("Z-tau"), model = c("constant"), lags = c("short"))
```

When referring to the returns, the conclusion is the same as for the ADF test; the returns are stationary while the original series is not. 

Finally, we use the KPSS test to check stationarity of the series. The null hypothesis for this test states that the series is stationary. In the test below we have chosen to assume the deterministic component as a constant with a linear trend, and we have used short lags. Notice that the conclusion is the same with all different variations of assumptions for the test. 

```{r}
ixic.kpss<-ur.kpss(ixic, type = c("tau"), lags = c("short"))
summary(ixic.kpss)
```

Since this is a right-tailed test, the test-statistic is clearly sufficiently large to reject the null-hypothesis to the lowest significance level shown (0.01). Thus, we conclude that the series is non-stationary, as expected. The test below shows that the returns are stationary, in line with what we have concluded earlier, since we cannot find strong evidence against the null-hypothesis. 

```{r}
rendixic.kpss <- ur.kpss(rendixic, type = c("mu"), lags = c("short"))
summary(rendixic.kpss)
```

Conclusively, the original time series is not stationary, but the returns are stationary, which means that the returns will be used in the following analysis. We can be relatively certain that this is the case, since all three formal tests, as well as the informal tests, point to this conclusion. 

## Basic Statistical Properties of the Stationary Series

Some basic statistical properties of the stationary series, the returns, are shown below. 

```{r, echo = F}
basicStats(rendixic)
```

```{r, echo = F}
par(mfrow=c(1,1),font=2,font.lab=4,font.axis=2,las=1)
hist(rendixic,breaks=100,freq=F, main = 'Histogram of the Returns')
curve(dnorm(x, mean=mean(rendixic), sd=sd(rendixic)), col=2, add=T)
```

It becomes apparent that the series is leptokurtic, both from the kurtosis value and from the histogram. The superposed red curve is a Gaussian distribution with empirical mean and standard error according to the returns of IXIC. Moreover, the skewness is negative, which means that the distribution of the returns is heavy-tailed in the left tail. This is also apparent from the histogram above. Without any further comments, the rest of the statistical properties may be interesting to have in mind. 

## Identification, Estimation and Diagnostics of a Model for the Mean

The autocorrelation functions of the returns are plotted below. 

```{r, echo = F}
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
acf(rendixic,ylim=c(-1,1),main="rendixic")
pacf(rendixic,ylim=c(-1,1),main="rendixic")
```

Note that the third coefficient of both ACF and PACF seems to be non-significant, which might be a hint to what order of model would be fitting. Notice also that both the ACF and the PACF have significant coefficients after the third lag; 6, 7, 8 and 9 seem to be significant. An ARMA of order 6, 7, 8 or 9 seems like a too large order of model to estimate, so we will try with smaller models instead, noting that the third coefficient is non-significant. The table below shows the BIC and the AIC for different orders of ARMA-models. The largest model that is considered is ARMA(3,2) (or ARMA(2,3)), since an ARMA(3,3) yields NaNs in the estimates. 

```{r, echo = F, results='hide'}
#ar(1)
model = arima(rendixic, order = c(1,0,0),include.mean = TRUE)
model$aic
BIC(model)
pnorm(c(abs(model$coef)/sqrt(diag(model$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#ma(1)
model2 = arima(rendixic, order = c(0,0,1),include.mean = TRUE)
model2 
pnorm(c(abs(model2$coef)/sqrt(diag(model2$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#arma(1,1) 
model3 = arima(rendixic, order = c(1,0,1),include.mean = TRUE)
model3 
pnorm(c(abs(model3$coef)/sqrt(diag(model3$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#ar(2) 
model4 = arima(rendixic, order = c(2,0,0),include.mean = TRUE)
model4 
pnorm(c(abs(model4$coef)/sqrt(diag(model4$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#MA(2) 
model5 = arima(rendixic, order = c(0,0,2),include.mean = TRUE)
model5 
pnorm(c(abs(model5$coef)/sqrt(diag(model5$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#arma(2,2)
model6 <- arima(rendixic, order = c(2,0,2),include.mean = TRUE)
model6
pnorm(c(abs(model6$coef)/sqrt(diag(model6$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#AR(3)
model7 <- arima(rendixic, order = c(3,0,0),include.mean = TRUE)
model7
pnorm(c(abs(model7$coef)/sqrt(diag(model7$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#MA(3)
model8 <- arima(rendixic, order = c(0,0,3),include.mean = TRUE)
model8
pnorm(c(abs(model8$coef)/sqrt(diag(model8$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#ARMA(3,2)
model9 <- arima(rendixic, order = c(3,0,2),include.mean = TRUE)
model9
pnorm(c(abs(model9$coef)/sqrt(diag(model9$var.coef))), mean=0, sd=1, lower.tail=FALSE)
```

Note that all models we have estimated here have significant coefficient estimates to a predetermined significance level of $\alpha = 0.05$.

```{r, echo = F}
models <- c("AR(1)", "MA(1)", "ARMA(1,1)", "AR(2)", "MA(2)", "ARMA(2,2)", "AR(3)", "MA(3)", "ARMA(3,2)") 
aics <- c(model$aic, model2$aic, model3$aic, model4$aic, model5$aic, model6$aic, model7$aic, model8$aic, model9$aic)
bics <- c(BIC(model), BIC(model2), BIC(model3), BIC(model4), BIC(model5), BIC(model6), BIC(model7), BIC(model8), BIC(model9))
df.arma <- cbind("Model" = models, "BIC" = bics, "AIC" = aics)
knitr::kable(df.arma, caption = "AIC and BIC of different estimated models for the returns of IXIC")
```

The table above clearly shows that ARMA(2,2) yields the lowest AIC and BIC. The estimated model ARMA(2,2) is shown below

```{r}
(mean.model <- arima(rendixic, order = c(2,0,2),include.mean = TRUE))
pnorm(c(abs(mean.model$coef)/sqrt(diag(mean.model$var.coef))), mean=0, sd=1, lower.tail=FALSE)
residuals <- mean.model$residuals
```

Some model diagnostics have to be done to check if the model is adequate. We must check if the model is stationary. The inverse roots of the characteristic polynomial of AR and MA are plotted.

```{r, echo = F}
plot(mean.model)
```

DO WE WANT ALL OF THE INVERSE ROOTS TO FALL INSIDE, FOR BOTH THE AR AND THE MA PROCESS? The stationarity condition for the AR-process is satisfied, since the roots have absolute values greater than one. Moreover, the invertibility condition holds for the MA process, since the roots of this process also have absolute values greater than one. Thus, the model is stationary. 

The residuals of the model are analyzed next. 

```{r, echo = F}
tsdiag(mean.model)
```

There are no significant coefficients in the autocorrelation function, which suggests that the model has adequately captured the information in the data. Moreover, the Ljung-Box statistic $p$-values are all relatively large, which means that we will not reject the Ljung-Box null hypothesis. This further suggests that the residuals are not correlated and we have found a model that seems reasonable in this regard. 

Next, a QQ-plot of the theoretical normal quantiles, and the residuals themselves (not standardized), is plotted.

```{r, echo = F}
par(mfrow = c(1,1),font=2,font.lab=4,font.axis=2,las=1)
qqnorm(residuals)
qqline(residuals, datax = FALSE)
```

Also, the residuals of the estimated model are plotted. 

```{r, echo = F}
plot(residuals)
title (main="Residuals")
```

The Jarque-Bera Normality test is also applied. 

```{r}
normalTest(residuals,method="jb")
```

It is apparent that the residuals have heavy tails. It is not reasonable to assume normality of the residuals, an argument that the Jarque-Bera Normality test further substantiates because its null hypothesis of normality is rejected following the very small $p$-value. 

## Identification, Estimation and Diagnostics of a Model for the Variance

First we test for ARCH effects using the residuals of the mean model. 

```{r}
residuals2 <- residuals^2
```

```{r, echo = F}
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
acf(residuals2,ylim=c(-1,1),main="Squared Residuals") 
pacf(residuals2,ylim=c(-1,1),main="Squared Residuals")
```

```{r, eval = T}
par(mfrow=c(1,1))
# Har acf ovenfor noe sammenheng med plottet nedenfor (se slides 33++ i Volatility Models)?
# For de ser veldig like ut. Men vet ikke helt om residuals^2 fra modellen har noe med dette å gjøre?
# Mulig det er fordi vi estimerer mean vha ARMA-modellen og trekker den fra + kvadrat, noe vi egt gjør direkte fra serien nedenfor!?
acf((rendixic-mean(rendixic))^2, ylim = c(-1,1))
```

As seen in the ACF and PACF of the squared residuals, they are clearly presenting autocorrelation, i.e. there are ARCH effects present. 

```{r}
Box.test(residuals2,lag=1,type='Ljung')
```

```{r, eval = F}
Box.test(residuals2,lag=5,type='Ljung')
Box.test(residuals2,lag=15,type='Ljung')
```

The argument is further substantiated by the Ljung-Box tests, where only the first result is shown, since they all lead to the conclusion that the squared residuals are correlated, because the null hypothesis is rejected. Thus, it is relevant to identify and estimate a model for the volatility. Joint estimation of the mean and volatility equations, for different types of models, is done in the following. 

Now over to estimation of GARCH models for the variance of the returns. First we estimate a ARMA(2,2)-GARCH(1,1) with a t-student distribution. WHY CHOOSE A T-STUDENT OVER A NORMAL (OR VICE VERSA)?

```{r}
spec1 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1,1)), 
            mean.model=list(armaOrder=c(2,2)), distribution.model = "std")
(m <- ugarchfit(spec = spec1, data = rendixic))
m.AIC <- -6.4605
```

We observe that all the parameter estimates are significant to a 5\% significance level. Moreover, we note that the condition of positivity holds, because $\hat{\alpha}_1 > 0$ and $\hat{\beta}_1 > 0$, where we follow the standard statistical notation of a hat indicating an estimate. Also, we note that the condition of stationarity holds, because $\hat{\alpha}_1 + \hat{\beta}_1 < 1$. We record the AIC of this first model in order to compare to other models later. The ACF of the residuals, plotted below, shows that the residuals do not present any autocorrelation (before moving to around 15 lags, which is a large number of lags), which indicates that this model has modeled the data in a sufficient or reasonable way. However, note that the $p$-values of the `Weighted Ljung-Box Test on Standardized Residuals` are quite large, which means we cannot reject the null hypothesis of no serial correlation for the different lags. This can be noted as a disadvantage of the first proposed model. 

```{r, echo = F}
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
plot(m, which = 10)
plot(m, which = 11)
```

<!-- The estimated volatilities from this first model, together with the returns, are shown in the plots below. The general behaviour seems to match relatively well, i.e. the movements in the two plots coincide relatively well.  -->

<!-- ```{r} -->
<!-- par(mfcol=c(2,1))  # Show volatility and returns simultaneously. -->
<!-- plot(v_anualized) -->
<!-- plot(rendixic)  -->
<!-- ``` -->

Next we will fit a ARMA(2,2)-GJR-GARCH(1,1) model, assuming a t-distribution. 

```{r}
spec.mgjr <- ugarchspec(variance.model=list(model="gjrGARCH", garchOrder = c(1,1)),
                    mean.model=list(armaOrder=c(2,2)), distribution.model = "std")
(mgjr <- ugarchfit(spec = spec.mgjr, data = rendixic))
```

The stationarity conditions hold, since $\hat{\alpha}_1 + \hat{\beta}_1 + \frac12 \hat{\gamma} \approx$ `r round(0.823974 + 0.5*0.260477, 3)` $<1$. However, the positivity condition does not seem to hold, since $\hat{\alpha} = 0 \ngeq 0$. Thus, the GJR-GARCH based model will not be used, even though the AIC is lower and the residuals do not present any autocorrelation according to plots. Moreover, the several of the ARMA-parameter coefficient estimates are not significant to a reasonable level, which is found to be the case no matter what `armaOrder` is used in the estimation. 

<!-- The residuals for the second model are plotted below.  -->

<!-- ```{r} -->
<!-- par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1) -->
<!-- plot(mgjr, which = 10) -->
<!-- plot(mgjr, which = 11) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- vgjr <- sigma(mgjr) # Estimated volatility.  -->
<!-- vgjr_anualized <- (250)^0.5*vgjr -->
<!-- par(mfcol=c(2,1)) -->
<!-- plot(vgjr_anualized) -->
<!-- plot(rendixic)  -->
<!-- ``` -->

Next, we will fit an ARMA(2,2)-EGARCH model. 
 
```{r}
spec.egarch <- ugarchspec(variance.model = list(model="eGARCH", garchOrder = c(1,1)), 
                    mean.model = list(armaOrder=c(2,2)), distribution.model = "std")
(m.egarch <- ugarchfit(spec = spec.egarch, data = rendixic))
m.egarch.AIC <- -6.4958
```

All estimated parameters are significant to a level of $\alpha = 0.05$. For this model, we do not require positivity of the GARCH parameter estimates. WHAT ABOUT STATIONARITY, DO WE REQUIRE THIS? The residual plots below do not show any autocorrelation before moving to a large number of lags, which is a good sign that the model has modelized the data adequately. Moreover, the AIC for this model is the smallest value thus far. Because of this, we would prefer this model. Note that $\gamma > 0$, which should mean that the positive news have a larger effect on the news compared to the negative news, which does not make sense here, when looking at the news impact curve. THIS IS STRANGE, I DO NOT UNDERSTAND THIS!? ASK PROFE!
DETTE ER MOTSATT DEFINERT I R VIRKER DET SOM! EN POSITIV GAMMA ER DET SAMME SOM EN NEGATIV I LIGNINGENE, I.E. EN POSITIV GAMMA GIR STØRRE EFFEKT FOR DE NEGATIVE NYHETENE ENN DE POSITIVE, SOM VI SER AV PLOTTET!

```{r, echo = F}
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
plot(m.egarch, which = 10)
plot(m.egarch, which = 11)
par(mfrow=c(1,1),font=2,font.lab=4,font.axis=2,las=1)
```

Next, we fit and ARMA(2,2)-IGARCH model. 

```{r}
spec.igarch <- ugarchspec(variance.model=list(model="iGARCH", garchOrder = c(1,1)), 
                    mean.model=list(armaOrder=c(2,2)), distribution.model = "std")
(m.igarch <- ugarchfit(spec=spec.igarch,data=rendixic))
m.igarch.AIC <- -6.4602
```

The residuals for the iGARCH look alright, but the AIC is lower for the EGARCH based model. Hence, I will not bother considering the other properties. 

```{r, echo = F}
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
plot(m.igarch, which = 10)
plot(m.igarch, which = 11)
```

Thus, I would conclude that the best model out of the four fitted is the EGARCH based model, since it satisfies its conditions and has the lowest AIC. 

## Grafic and Interpretation of the Estimated Series of Volatility

The estimated (annualized) series of volatility for the ARMA(2,2)-EGARCH model is plotted, alongside the returns and the absolute values of the returns. 

```{r, warning=FALSE, echo = F}
v <- sigma(m.egarch) 
v_anualized <- (250)^0.5*v
returnabs <- abs(rendixic)
tibble("Date" = as.Date(Ixic[-c(1),1]), "Annualized-Vol." = v_anualized[,1], "Returns" = rendixic, "Abs.-Returns" = returnabs) %>% 
gather(key = distribution, value = value, -Date)%>% 
ggplot(aes(x = Date, y = value)) +
  geom_line() +
  theme_minimal() +
  scale_x_date(date_labels = "%Y %b %d", date_breaks = "1 year") +
  facet_grid(rows = vars(distribution), scales = "free")
```

The general behaviour seems to match relatively well, i.e. the movements in the plots coincide relatively well. Days with larger (absolute) returns coincide with days with larger estimated volatilities. Note that we cannot compare the absolute values in plot however. 

```{r, warning = F, echo = F, eval = F}
returnsabs <- abs(rendixic)
tibble("date" = as.Date(Ixic[-c(1),1]), "v_anualized" = v_anualized[,1], returnsabs) %>% 
gather(key = distribution, value = value, -date)%>% 
ggplot(aes(x = date, y = value)) +
  geom_line() +
  theme_minimal() +
  scale_x_date(date_labels = "%Y %b %d", date_breaks = "1 year") +
  facet_wrap(~distribution, nrow = 2, scales = "free")
```

```{r, echo = F, eval = F}
par(mfrow=c(1,1),font=2,font.lab=4,font.axis=2,las=1)
time <-  data.frame(returnsabs, v)
ts.plot(time,gpars= list(xlab="time", ylab=",", col = 1:ncol(time)))
legend("topleft", c("returnsabs","v"), lty=c(1,1), col=c("black","red"), cex=0.6)
```

## Grafic and Interpretation of the News Impact Curve

The news impact curve for our chosen model is shown below. Since the standard GARCH model does not take the leverage effect into effect, the news impact curve is symmetric. 

```{r}
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
nie <- newsimpact(z = NULL, m.egarch)
plot(nie$zx, nie$zy, ylab=nie$yexpr, xlab=nie$xexpr, type="l", main = "News Impact Curve EGARCH")
ni <- newsimpact(z = NULL, m)
plot(ni$zx, ni$zy, ylab=ni$yexpr, xlab=ni$xexpr, type="l", main = "News Impact Curve sGARCH")
```

The EGARCH based model considers the leverage effect, which is why the news impact curve is non-symmetric. This curve indicates that the volatility is impacted to a higher degree by negative news compared to the impact on the volatility following positive news, which decreases as the positivity of the news increases THIS IS VERY STRANGE!! ASK PROFE!

## Volatility Predictions and Interpretations

Volatility is predicted while leaving out the last 10 observations when estimating the ARMA(2,2)-EGARCH model. The prediction is done 10 steps ahead into the future, first statically. 

```{r}
m.egarch.pred <- ugarchfit(spec = spec.egarch, data = rendixic, out.sample = 10)
forc <- ugarchforecast(m.egarch.pred, n.ahead=10, n.roll= 0) 
```

```{r, echo = F, fig.height = 7}
par(mfrow=c(2,1),font=2,font.lab=4,font.axis=2,las=1)
plot(forc, which = 1)
plot(forc, which = 3)
```

```{r, echo = F}
cat("Long Run Unconditional Variance: ",uncvariance(m.egarch.pred)^0.5)
```

As we can see from the uppermost plot, these static predictions (for the mean) 10 steps ahead are relatively useless. ALSO LOOKS LIKE THE FORECAST OF THE VARIANCE WILL GO TOWARDS THE UNCONDITIONAL VARIANCE. NOT SURE IN WHAT SITUATIONS THIS SHOULD HAPPEN THOUGH, ASK PROFE PERHAPS!

Next, let us predict 10 steps into the future with a rolling window. We reestimate the model at each time step and estimate one step into the future after each reestimation. After doing this 10 times, we have effectively predicted 10 days into the future. 

```{r}
forc2 <- ugarchforecast(m.egarch.pred, n.ahead=1, n.roll= 10)
```

```{r, echo = F, fig.height = 7}
par(mfrow=c(2,1),font=2,font.lab=4,font.axis=2,las=1)
plot(forc2, which = 2)
plot(forc2, which = 4)
```

```{r, echo = F}
cat("Long Run Unconditional Variance: ",uncvariance(m.egarch.pred)^0.5)
```

The predictions are still lousy, as can be seen from the predictions of the mean in the uppermost plot. However, from the second plot, it looks like the predictions of the variance are somewhat following similar movements as the absolute value of the series; when the absolute value of the series hits a spike, the predictions of the volatility increase as well. 

The same type of movement can be seen when predicting with a rolling window 100 steps into the future, as done next. 

```{r}
m.egarch.pred2 <- ugarchfit(spec = spec.egarch, data = rendixic, out.sample = 100)
forc3 <- ugarchforecast(m.egarch.pred2, n.ahead=1, n.roll= 100)
```

```{r, echo = F, fig.height = 7}
par(mfrow=c(2,1),font=2,font.lab=4,font.axis=2,las=1)
plot(forc3, which = 2)
plot(forc3, which = 4)
```

```{r, echo = F}
cat("Long Run Unconditional Variance: ",uncvariance(m.egarch.pred2)^0.5)
```

## Calculations via Historical Volatility and EWMA

Historical volatility is calculated below. The historical volatility has been calculated using Simple Moving Average (SMA) over different time periods

$$
\sigma_t^2 = \frac1k\sum_{i=1}^kr_{t-1}^2.
$$

The results for different time periods $k$ are shown below. 

```{r, echo = F}
Fechas<-as.Date(Ixic[,1])
Fechas<-Fechas[-1] # Eliminate the first date, since it was lost when calculating returns. 

vol.hist20 <- SMA(rendixic^2, n=20) 
Fechas2<-Fechas[21:2556] 

vol.hist80 <- SMA(rendixic^2, n=80) 
Fechas3<-Fechas[81:2556]

vol.hist160 <- SMA(rendixic^2, n=160) 
Fechas4<-Fechas[161:2556]

vol.hist240 <- SMA(rendixic^2, n=240) 
Fechas5<-Fechas[241:2556]
 
par(mfrow=c(2,2), cex=0.6, mar=c(2,2,3,1))
plot(Fechas2, vol.hist20, type="l", ylab='variance', xlab="time", main='1 month moving average')
plot(Fechas3, vol.hist80, type="l", ylab='variance', xlab="time", main='4 month moving average')
plot(Fechas4, vol.hist160, type="l", ylab='variance', xlab="time", main='8 month moving average')
plot(Fechas5, vol.hist240, type="l", ylab='variance', xlab="time", main='1 year moving average')
par(mfrow=c(1,1),font=2,font.lab=4,font.axis=2,las=1)
```

As is apparent from the plots, the volatility pattern is highly dependent on the $k$, i.e. the number of observations used to calculate the moving average. Moreover, we can see that the results are greatly affected by extreme values, especially when $k$ is small, which is clearly seen in the results for the 1 month moving average. The volatility pattern is smoother when $k$ is larger. Which of these values for $k$ gives the "best" results? This is difficult to answer. 

Following the calculations from historical volatility, the Exponentially Weighted Moving Average (EWMA) model is used to calculate volatility

$$
\sigma_t^2 = (1-\lambda)r_{t-1}^2 + \lambda\sigma_{t-1}^2 = (1-\lambda) \sum_{i = 1}^\infty\lambda^{i-1}r_{t-1}^2, \hspace{0.5em} 0<\lambda<1.
$$

Different values of the parameter $\lambda$ are used in order to see how the results depend on it. From the theoretical point of view, we know that the term $(1-\lambda)r_{t-1}^2$ determines the reaction of volatility to market events, i.e. the larger the term $(1-\lambda)$ the larger the reaction in the volatility stemming from yesterday's return. Moreover, the term $\lambda\sigma_{t-1}^2$ determines the persistence in volatility. In other terms, it decides how much of yesterday's volatility is allowed to persist to today's volatility: A larger value of $\lambda$ gives larger persistence. Thus, the EWMA model gives a trade-off between persistence and reaction in the volatility, depending on the value of $\lambda$. 

Some results from calculating the volatility using EWMA with different values of $\lambda$ are plotted. 

```{r, echo = F}
vol.ewma0.95 <- EWMA(rendixic^2, lambda = 0.05) # note: in EWMA lambda is actually 1-lambda
vol.ewma0.75 <- EWMA(rendixic^2, lambda = 0.25) # note: in EWMA lambda is actually 1-lambda
vol.ewma0.5 <- EWMA(rendixic^2, lambda = 0.5) # note: in EWMA lambda is actually 1-lambda
vol.ewma0.25 <- EWMA(rendixic^2, lambda = 0.75) # note: in EWMA lambda is actually 1-lambda

par(mfrow=c(2,2), cex=0.6, mar=c(2,2,3,1))
plot(Fechas, vol.ewma0.95, type="l", ylab='variance', main=TeX(r'(EWMA, $\lambda = 0.95$)', bold = TRUE))
plot(Fechas, vol.ewma0.75, type="l", ylab='variance', main=TeX(r'(EWMA, $\lambda = 0.75$)', bold = TRUE))
plot(Fechas, vol.ewma0.5, type="l", ylab='variance', main=TeX(r'(EWMA, $\lambda = 0.5$)', bold = TRUE))
plot(Fechas, vol.ewma0.25, type="l", ylab='variance', main=TeX(r'(EWMA, $\lambda = 0.25$)', bold = TRUE))
```

As we can see from the plots, the larger values of $\lambda$ give smoother plots, since the persistence is larger, while the smaller values of $\lambda$ give a more reactive or non-smooth volatility pattern, since the persistence of the volatility is much lower in these cases. Comparing to the results obtained when using the historical volatility, all the volatility patterns obtained with EWMA are more non-smooth than the former, being most similar to the 1 month moving average. Note also that the choice of $\lambda$ seems somewhat arbitrary in this case (similar to the choice of $k$ for historical volatility), as it is difficult to be certain about the best choice of the parameter. 

Doing a quick comparison between these two models and the results from the EGARCH model, it looks like the EWMA model with $\lambda = 0.75$ gives a relatively similar volatility pattern, whereas the 1 month moving average (which is the one among the four models that is most similar to the results from EGARCH) is lacking in comparison. 

```{r, echo = F}
par(mfrow=c(1,1), cex=0.6, mar=c(2,2,3,1))
variance <- v^2
comparison <- data.frame(variance, vol.ewma0.75)
ts.plot(comparison,gpars= list(ylab=",", col = 1:ncol(comparison), xaxt = "n"))
legend("topleft", c("E-GARCH",TeX(r'(EWMA, $\lambda = 0.75$)')), lty=c(1,1), col=c("black","red"))
axis(1, at=1:6)#, labels=c(2012, 2014, 2016, 2018, 2020, 2022))

variance2 <- variance[21:2555]
volhist <- vol.hist20[1:2535]
comparison <- data.frame(variance2, volhist)
ts.plot(comparison,gpars= list(ylab=",", col = 1:ncol(comparison), xaxt = "n"))
legend("topleft", c("E-GARCH","1 month moving average"), lty=c(1,1), col=c("black","red"))
axis(1, at=1:6)#, labels=c(2012, 2014, 2016, 2018, 2020, 2022))
```

NOTE THAT THE TIMES ON THE X-AXIS ARE FUCKED UP. TRY TO FIX THIS LATER! THIS IS THE CASE SEVERAL PLACES!

## Calculation and Interpretation of VaR 

Here we will calculate and interpret the Value at Risk (VaR) using estimated volatilities from several different models. First we use the variance-covariance method, calculating the VaR with a static forecast one time ahead, using the EGARCH model.

```{r VaR, cache = T}
forc <- ugarchforecast(m.egarch, n.ahead=1, n.roll= 0)
show(forc)
var5.garch <- - qnorm(0.95) * 0.01791
cat("VaR: ", show(var5.garch))
```

This value means that, with a confidence level of 95\%, the largest expected loss for tomorrow in our index is $\approx 2.95$\%. In other terms, the probability of the return tomorrow being lower than -2.95\% is 5\%. 

Next we calculate the VaR with a rolling window dynamic forecast, using the EGARCH model, with a significance level of 5\%.

```{r, cache = T}
var.t <-  ugarchroll(spec.egarch, data = rendixic, n.ahead = 1, forecast.length = 50, 
              refit.every = 10,  refit.window = "rolling",
              calculate.VaR = TRUE, VaR.alpha = 0.05)
```

```{r, echo = F}
plot(var.t, which = 4, VaR.alpha = 0.05)
```

```{r}
report(var.t, VaR.alpha = 0.05)
```

The report above shows that our predefined level of 5\% significance is not kept, i.e. that the largest expected loss cannot be quantified at the 5\% significance level. Instead, the VaR is estimated to be 14%, which means that the probability of the return the next day being lower than the VaR is $\approx 14$% instead of 5%. In practice, this means that the company should set aside more funds than expected, in order to cover the predefined significance level of 5\%. 

LITT USIKKER PÅ DENNE TOLKNINGEN!

Next, we calculate the VaR using estimates of volatility from the EWMA model with $\lambda = 0.75$ and from the ARMA(2,2)-EGARCH model. 

```{r ewma/VaR, cache = T}
var5.ewma  <-  - qnorm(0.95) * sqrt(vol.ewma0.75)
var5.egarch <- - qnorm(0.95) * v
var1.ewma  <-  - qnorm(0.99) * sqrt(vol.ewma0.75)
var1.egarch <-  - qnorm(0.99) * v
```

```{r, echo = F}
par(mfrow=c(2,2), cex=0.6, mar=c(2,2,3,1))
plot(Fechas, rendixic,type="l", main ="5% VaR EWMA")
lines(Fechas, var5.ewma, col = "blue")
plot(Fechas, rendixic,type="l", main ="5% VaR E-GARCH(1,1)")
lines(Fechas,var5.egarch, col ="blue")
plot(Fechas, rendixic,type="l", main ="1% VaR EWMA")
lines(Fechas, var1.ewma, col = "red")
plot(Fechas, rendixic, type="l", main ="1% VaR E-GARCH(1,1)")
lines(Fechas, var1.egarch, col ="red")
```

The plots above show the estimated VaR's with the two different models, at two different significance levels (5\% shown in blue and 1\% shown in red), plotted together with the returns. To the naked eye it looks like the returns don't sink below the 1\% VaR very often, while they sink below the 5\% VaR somewhat more often, but still rarely. To quantify this, we calculate the fraction of the sample where the loss in returns exceeds each of the significance levels for the two models. USIKKER PÅ DENNE TOLKNINGEN OGSÅ!!?

```{r, echo = F}
cat("Fraction of sample where loss exceeds 5% VaR for EWMA: ", sum(rendixic < var5.ewma)/length(rendixic)) 
cat("Fraction of sample where loss exceeds 5% VaR for EGARCH: ",sum(rendixic < var5.egarch)/length(rendixic)) 
cat("Fraction of sample where loss exceeds 1% VaR for EWMA: ",sum(rendixic < var1.ewma)/length(rendixic))
cat("Fraction of sample where loss exceeds 1% VaR for EGARCH: ",sum(rendixic < var1.egarch)/length(rendixic)) 
```

In a case where we choose a significance level of $\alpha = 0.01$ we can see that the EWMA model with $\lambda = 0.75$ overestimates the risk since STEMMER DETTE HER! SJEKK MED EKSEMPELET HUN BRUKTE MED RENDIBEX!

This is a good example of how a GARCH model is advantegeous compared to an EWMA model, since we use the data to estimate the model ("the data talks") and we need not to set a hyperparameter like $\lambda$ which the results depend largely on. 

Redoing the calculations with the EWMA model with $\lambda = 0.95$ instead gives the fractions

```{r, echo = F}
var5.ewma  <-  - qnorm(0.95) * sqrt(vol.ewma0.95)
var1.ewma  <-  - qnorm(0.99) * sqrt(vol.ewma0.95)
```

```{r}
cat("Fraction of sample where loss exceeds 5% VaR for EWMA: ", sum(rendixic < var5.ewma)/length(rendixic)) 
cat("Fraction of sample where loss exceeds 1% VaR for EWMA: ",sum(rendixic < var1.ewma)/length(rendixic))
```

# Multivariate Analysis

## Multivariate DCC GARCH

In order to solve this problem I have chosen the stock of Stratus Properties Inc. (STRS), which is one of the [top 30 components](https://finance.yahoo.com/quote/%5EIXIC/components?p=%5EIXIC) of the NASDAQ Composite Index. Similarly to the IXIC-data, this data has been downloaded in a csv-file directly from the Yahoo Finance website. The adjusted close of the time series is plotted below. 

```{r}
# Get STRS stock data and redo analysis for this stock. 
STRS <- read.csv("STRS.csv")
head(STRS)
any(is.na(STRS))
str(STRS)
strs <- STRS[,6]
head(strs)

tibble("date" = as.Date(STRS[,1]), strs) %>% ggplot(aes(x = date, y = strs)) +
    geom_line() +
    theme_minimal() +
    scale_x_date(date_labels = "%Y %b %d", date_breaks = "1 year") +
    ggtitle("Stratus Properties Inc. (STRS)")
```


The returns of STRS are calculated and plotted below. 

```{r}
rendstrs <- diff(log(strs))
tibble("date" = as.Date(STRS[-c(1),1]), rendstrs) %>% ggplot(aes(x = date, y = rendstrs)) +
    geom_line() +
    theme_minimal() +
    scale_x_date(date_labels = "%Y %b %d", date_breaks = "1 year") +
    ggtitle("Returns of Stratus Properties Inc. (STRS)")
```

Analysis of stationarity shows that this series is integrated of order 1 as well, similarly to IXIC. Thus, we work with the returns of the series instead of the series itself. 

```{r analysisOfSTRS, eval = F, echo = F}
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1) 
acf(strs,ylim=c(-1,1),main="strs")
pacf(strs,ylim=c(-1,1),main="strs")

Box.test(strs, lag = 1, type = c("Ljung-Box"))
Box.test(strs, lag = 5, type = c("Ljung-Box"))
Box.test(strs, lag = 10, type = c("Ljung-Box"))
Box.test(strs, lag = 15, type = c("Ljung-Box"))
Box.test(strs, lag = 20, type = c("Ljung-Box"))

strs.df<-ur.df(strs, type = c("trend"), lags=20, selectlags = c("BIC"))
summary(strs.df)   
plot(strs.df)

rendstrs.df<-ur.df(rendstrs, type = c("none"), lags=20, selectlags = c("BIC"))
summary(rendstrs.df) 
plot(rendstrs.df)

strs.pp<-ur.pp(strs, type = c("Z-tau"), model = c("trend"), lags = c("long"))
summary(strs.pp)  

rendstrs.pp<-ur.pp(rendstrs, type = c("Z-tau"), model = c("trend"), lags = c("long"))
summary(rendstrs.pp)  

strs.kpss<-ur.kpss(strs, type = c("tau"), lags = c("short"))
summary(strs.kpss)

rendstrs.kpss<-ur.kpss(rendstrs, type = c("tau"), lags = c("short"))
summary(rendstrs.kpss)

basicStats(rendstrs)

par(mfrow=c(1,1),font=2,font.lab=4,font.axis=2,las=1)
hist(rendstrs,breaks=100,freq=F, main = 'Histogram of the Returns')
curve(dnorm(x, mean=mean(rendstrs), sd=sd(rendstrs)), col=2, add=T)

par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
acf(rendstrs,ylim=c(-1,1),main="rendstrs")
pacf(rendstrs,ylim=c(-1,1),main="rendstrs")

#ar(1)
model = arima(rendstrs, order = c(1,0,0),include.mean = TRUE)
model$aic
BIC(model)
pnorm(c(abs(model$coef)/sqrt(diag(model$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#ma(1)
model2 = arima(rendstrs, order = c(0,0,1),include.mean = TRUE)
model2 
pnorm(c(abs(model2$coef)/sqrt(diag(model2$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#arma(1,1) 
model3 = arima(rendstrs, order = c(1,0,1),include.mean = TRUE)
model3 
pnorm(c(abs(model3$coef)/sqrt(diag(model3$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#ar(2) 
model4 = arima(rendstrs, order = c(2,0,0),include.mean = TRUE)
model4 
pnorm(c(abs(model4$coef)/sqrt(diag(model4$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#MA(2) 
model5 = arima(rendstrs, order = c(0,0,2),include.mean = TRUE)
model5 
pnorm(c(abs(model5$coef)/sqrt(diag(model5$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#arma(2,2)
model6 <- arima(rendstrs, order = c(2,0,2),include.mean = TRUE)
model6
pnorm(c(abs(model6$coef)/sqrt(diag(model6$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#AR(3)
model7 <- arima(rendstrs, order = c(3,0,0),include.mean = TRUE)
model7
pnorm(c(abs(model7$coef)/sqrt(diag(model7$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#MA(3)
model8 <- arima(rendstrs, order = c(0,0,3),include.mean = TRUE)
model8
pnorm(c(abs(model8$coef)/sqrt(diag(model8$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#arma(2,3)
model9 <- arima(rendstrs, order = c(3,0,3),include.mean = TRUE)
model9
pnorm(c(abs(model9$coef)/sqrt(diag(model9$var.coef))), mean=0, sd=1, lower.tail=FALSE)

#arma(4,3)
model10 <- arima(rendstrs, order = c(4,0,3),include.mean = TRUE)
model10
pnorm(c(abs(model10$coef)/sqrt(diag(model10$var.coef))), mean=0, sd=1, lower.tail=FALSE)

models <- c("AR(1)", "MA(1)", "ARMA(1,1)", "AR(2)", "MA(2)", "ARMA(2,2)", "AR(3)", "MA(3)", "ARMA(3,3)", "ARMA(4,3)") 
aics <- c(model$aic, model2$aic, model3$aic, model4$aic, model5$aic, model6$aic, model7$aic, model8$aic, model9$aic, model10$aic)
bics <- c(BIC(model), BIC(model2), BIC(model3), BIC(model4), BIC(model5), BIC(model6), BIC(model7), BIC(model8), BIC(model9), BIC(model10))
df.arma <- cbind("Model" = models, "BIC" = bics, "AIC" = aics)
knitr::kable(df.arma, caption = "AIC and BIC of different estimated models for the returns of STRS")
# Looks like ARMA(3,3) is the best, because of the lowest AIC (ARMA(4,4) produces NaNs, so cannot keep testing models with more params).
# However, ar3 is not significant!!

# Choose the MA(1) for lowest BIC and second lowest AIC (and significant coefficients).

mean.model <- arima(rendstrs, order = c(0,0,1),include.mean = TRUE)
mean.model
pnorm(c(abs(mean.model$coef)/sqrt(diag(mean.model$var.coef))), mean=0, sd=1, lower.tail=FALSE)
plot(mean.model) # It is stationary. Alles gut. 
tsdiag(mean.model) # No significant coefficients in the ACF. GREAT!

par(mfrow = c(1,1),font=2,font.lab=4,font.axis=2,las=1)
qqnorm(mean.model$residuals)
qqline(mean.model$residuals, datax = FALSE)

plot(mean.model$residuals)
title (main="Residuals")
normalTest(mean.model$residuals,method="jb") # Unreasonable to assume normality of the residuals. 

# Test for ARCH effects. 
residuals <- mean.model$residuals
residuals2 <- residuals^2
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
acf(residuals2,ylim=c(-1,1),main="Squared Residuals") 
pacf(residuals2,ylim=c(-1,1),main="Squared Residuals")
Box.test(residuals2,lag=1,type='Ljung')
Box.test(residuals2,lag=5,type='Ljung')
Box.test(residuals2,lag=10,type='Ljung')
Box.test(residuals2,lag=15,type='Ljung')
Box.test(residuals2,lag=20,type='Ljung')
# There clearly are ARCH effects here!

# Testing with different GARCH models for the volatility. 
spec1 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1,1)), mean.model=list(armaOrder=c(0,1)), distribution.model = "std")
(m <- ugarchfit(spec = spec1, data = rendstrs)) # Positivity and stationarity holds. 
m.aic <- -4.7716
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
plot(m, which = 10)
plot(m, which = 11)

spec.mgjr <- ugarchspec(variance.model=list(model="gjrGARCH", garchOrder = c(1,1)), mean.model=list(armaOrder=c(0,1)), distribution.model = "std")
(mgjr <- ugarchfit(spec = spec.mgjr, data = rendstrs)) # Everything holds!
mgjr.aic <- -4.7723 # Smaller AIC!
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
plot(mgjr, which = 10)
plot(mgjr, which = 11)

spec.egarch <- ugarchspec(variance.model = list(model="eGARCH", garchOrder = c(1,1)), mean.model = list(armaOrder=c(0,1)), distribution.model = "std")
(m.egarch <- ugarchfit(spec = spec.egarch, data = rendstrs)) # Do we expect stationarity and positivity?
megarch.aic <- -4.7801 # If stationarity is not needed, then this is the best model!

par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
plot(m.egarch, which = 10)
plot(m.egarch, which = 11)
plot(m.egarch, which = 12)

spec.igarch <- ugarchspec(variance.model=list(model="iGARCH", garchOrder = c(1,1)), mean.model=list(armaOrder=c(0,1)), distribution.model = "std")
(m.igarch <- ugarchfit(spec=spec.igarch,data=rendstrs))
m.igarch.aic <- -4.7724 # Bigger AIC than EGARCH --> Egarch is better!
par(mfrow=c(1,2),font=2,font.lab=4,font.axis=2,las=1)
plot(m.igarch, which = 10)
plot(m.igarch, which = 11)
```

After doing a similar analysis of this series, I have come to the conclusion that an MA(1)-EGARCH is the best model to estimate its volatility. This model is used, together with the ARMA(2,2)-EGARCH from IXIC, to estimate the multivariate DCC GARCH model for these two series (both of which are estimated on the logarithmic returns, not on the series themselves). 

```{r}
returns <- cbind(rendixic,rendstrs) 

# dcc specification - GARCH(1,1) for conditional correlations con diferentes garch para cada serie, en alg?n caso no es modelo asim?trico
spec1 <- ugarchspec(mean.model = list(armaOrder = c(2,2)), variance.model = list(garchOrder = c(1,1), model = "eGARCH"), distribution.model = "std") 
spec2 <- ugarchspec(mean.model = list(armaOrder = c(0,1)), variance.model = list(garchOrder = c(1,1), model = "eGARCH"), distribution.model = "std") 
dcc.garch11.spec <- dccspec(uspec = multispec(c(spec1, spec2)), dccOrder = c(1,1), distribution = "mvnorm")
(dcc.fit <- dccfit(dcc.garch11.spec, data = returns))
```

WHAT ASSUMPTIONS NEED TO BE FULFILLED FOR THIS MODEL!?

## Estimated Correlation and News Impact Surface

The plot below shows the conditional standard error estimated from the model (in blue) and the realized absolute returns (in grey). 

```{r}
plot(dcc.fit, which = 2)
```

From this plot we can gather that the model has made good estimations of the standard error, because the behaviour of the graphs are similar. Note that we cannot conclude anything based on the absolute values of the quantities; we are only interested in the shape or the behaviour of the quantities. 

The plot below shows the conditional correlation estimated from the model. 

```{r}
cor1 <- rcor(dcc.fit) 
par(mfrow=c(1,1),font=2,font.lab=4,font.axis=2,las=1)
plot(as.Date(STRS[-1,1]),cor1[1,2,], type='l', main="Correlacion between IXIC and STRS")
```

The correlation is plotted alongside the two original time series below. 

```{r}
tibble("date" = as.Date(Ixic[-1,1]),"IXIC" = ixic[-1], "STRS" = strs[-1], "cor" = cor1[1,2,]) %>% 
gather(key = distribution, value = value, -date)%>% 
ggplot(aes(x = date, y = value)) +
  geom_line() +
  theme_minimal() +
  scale_x_date(date_labels = "%Y %b %d", date_breaks = "1 year") +
  facet_wrap(~distribution, nrow = 3, scales = "free")
```

Just by looking at the two time series side by side, they look to move in a similar fashion. The peak in the correlation between the two series, which corresponds to a correlation of about 0.45, took place in the beginnin of 2020, when both prices fell, most likely because of the COVID-pandemic. 

OTHER THAN THAT, I AM NOT QUITE CERTAIN THAT I AM ABLE TO INTERPRET ANYTHING ELSE FROM THE PLOTS. 

The news impact surface is plotted below. 

```{r, results='hide'}
nisurface(dcc.fit, type="cor")
```

From this we can learn that 

* Simultaneous negative news in both series lead to the largest increase in correlation ????.
* Simultaneous positive news in both series lead to an increase in correlation ??? as well, but not to the same extent that the negative shocks do. This shows that the leverage effect has been taken into account in the model. 
* Negative news in one of the series and positive news in the other leads to a negative correlation ???, which (again) shows that the leverage effect is taken into account, since the negative news clearly get a larger weight than the positive news. 

STEMMER DET AT DET ER KORRELASJONEN SOM ENDRES I NEWS IMPACT CURVE OVER, OG IKKE VOLATILITY!?!?!

# Conclusions 

# QUESTIONS FOR PROFE: 

* Generelt: Er hun interessert i å se kode i rapporten, eller bare resulater med diskusjon? Hvor mye av output fra tester etc kan sløyfes?
* Correct to talk about returns, when they are calculated as diff(log(IXIC))?
* Spør om ekstra plot under 3.5!? Har (rendixic - mean(rendixic))^2 noe sammenheng med ACF av residuals^2? Estimador insesgado?
* In GARCH models: WHY CHOOSE A T-STUDENT OVER A NORMAL (OR VICE VERSA)?
* Is positivity needed for GJR-GARCH as well? Because this is used to reject the possiblity of using GJR-GARCH as a model for the variance!
* Positivity and stationarity assumptions for EGARCH? Looks like positivity is not needed! But is stationarity an assumption that needs to be fulfilled in the model?
* Value of gamma in EGARCH: Opposite of how we defined it? Se kommentarer i selve oppgaven (side 19/20).
* News impact curve for EGARCH: Veldig merkelig kurve, noe galt? Eller er dette noe som kan skje?
* When does the predicted volatility go towards the unconditional variance?
* Har noe trøbbel med tolkning av VaR (lese gjennom stoff først kanskje!). Noe jeg burde spørre om ift det?
